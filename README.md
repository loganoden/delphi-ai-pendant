# DELPHI: Wearable AI Assistant for the Visually Impaired

**Duration:** March 2025  
**Event:** MakeOHI/O 2025 – 99P Labs *Building the Future with On-Device LLMs* Challenge  
**Team Size:** 4 (Runner-Up Placement)  

---

## 📘 Overview
**DELPHI** is a wearable AI pendant prototype designed to assist visually impaired users by detecting faces and audibly identifying recognized individuals in real time, completely **on-device**.  
Developed in just **24 hours** during *MakeOHI/O 2025*, the project integrates multiple edge computing systems and lightweight AI models to deliver private, low-latency recognition and communication without cloud dependence.

---

## ⚙️ Technical Implementation
- **Distributed Edge System:** Utilized a **Raspberry Pi 5** for face detection and a **Raspberry Pi 4** for recognition, local LLM responses, and text-to-speech.  
- **Computer Vision:** Enhanced face detection accuracy through optimized face-cropping using **OpenCV** and **Ultralytics YOLO**.  
- **Local AI Models:** Integrated **TinyLlama** for on-device language understanding, **Piper TTS** for speech output, and **SpeechRecognition** for hands-free interaction.  
- **Performance:** Delivered full on-device inference with multi-identity support, reducing latency and ensuring privacy by eliminating cloud processing.

---

## 🧠 Skills and Technologies
**Languages & Libraries:** Python · OpenCV · Ultralytics YOLO · SpeechRecognition  
**Hardware:** Raspberry Pi 5 · Raspberry Pi 4  
**AI Tools:** TinyLlama · Piper TTS  
**Domains:** Edge Computing · Computer Vision · Machine Learning · Assistive Technology  
**Soft Skills:** Strategic Collaboration · Rapid Prototyping  

---

## 🏆 Results
Runner-up at **99P Labs’ Building the Future with On-Device LLMs Challenge** (*MakeOHI/O 2025*), recognized for innovation in edge-based AI and accessibility technology.

---

## 📄 License / Documentation
Future documentation and demo materials will be provided as development continues.
